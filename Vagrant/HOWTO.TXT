#!/bin/sh

###################################################
#
# Prereqs for Vagrant
#
###################################################

# You need the vagrant-reload provisioning plugin
$ vagrant plugin install vagrant-reload

$ vagrant up


###################################################
#
# Commands to re-create the Swarm cluster layer
#
###################################################

# Start the kv (for node discovery and libnetwork): 
docker -H=tcp://192.168.33.11:2375 run --restart=unless-stopped -d -p 8500:8500 -p 8400:8400 -p 8600:53/udp --name consul -h consul progrium/consul -server -bootstrap -ui-dir /ui
# web ui on:  http://192.168.33.11:8500/ui/

# Start the Swarm manager
docker -H=tcp://192.168.33.11:2375 run --restart=unless-stopped -d -p 3375:2375 swarm manage --discovery-opt="kv.path=docker/nodes" consul://192.168.33.11:8500/

# Create the overlay network
export DOCKER_HOST="tcp://192.168.33.11:3375"
docker network create --driver overlay mynet

# Registrator agent on every node
arr=("frontend01" "frontend02" "worker01" "pg" "master" "interlock") ; \
for NODE in "${arr[@]}"; do \
    docker run -d -e constraint:node==$NODE --net=mynet --volume=/var/run/docker.sock:/tmp/docker.sock gliderlabs/registrator:latest consul://192.168.33.11:8500/ ; \
done


###################################################
#
# Build instructions; same on AWS and Vagrant
#
###################################################

#
# To build web-vote-app:
#
cd web-vote-app
docker rmi web-vote-app
docker -H tcp://192.168.33.20:2375 build -t web-vote-app .
docker -H tcp://192.168.33.21:2375 build -t web-vote-app .
# [etc... build for each web node because `docker build` on swarm master won't put image on every machine]
docker -H tcp://192.168.33.99:2375 build -t web-vote-app .

#
# To build worker:
#
cd vote-worker
docker rmi vote-worker
docker -H tcp://192.168.33.200:2375 build -t vote-worker .
docker -H tcp://192.168.33.201:2375 build -t vote-worker .
# [etc... build for each worker node because `docker build` on swarm master won't put image on every machine]
docker -H tcp://192.168.33.249:2375 build -t vote-worker .

#
# To put redis image on each web node:
#
docker -H tcp://192.168.33.20:2375 pull redis
docker -H tcp://192.168.33.21:2375 pull redis
# [etc... for each web+redis node]
docker -H tcp://192.168.33.99:2375 pull redis

#
# To build results-app (which runs on pg machine)
#
docker -H tcp://192.168.33.251:2375 build -t results-app .


###################################################
#
# DNS setup:  do on machine from which you browse
#
###################################################

#
# Vagrant:  point votingapp.local -> interlock on .12
#
vi /etc/hosts
Add this line:
	192.168.33.12   votingapp.local

#
# AWS:  point votingapp.local -> public IP of interlock (found in Outputs tab of CloudFormation)
#
vi /etc/hosts
Add this line:
	<public IP of interlock>   votingapp.local


###################################################
#
# Start application containers
#
###################################################

#
# AWS and Vagrant:  start Interlock on dedicated machine .12 (but listening to swarm master on .11):
#
docker -H tcp://192.168.33.12:2375 run --restart=unless-stopped -p 80:80 --name interlock -d ehazlett/interlock --swarm-url tcp://192.168.33.11:3375 --plugin haproxy start

#
# To run web+redis containers:
#
export DOCKER_HOST="tcp://192.168.33.11:3375"

docker run --restart=unless-stopped --env="constraint:node==frontend01" -p 6379:6379 --name redis01 --net mynet -d redis  ## expose 6379 for debugging only!
docker run --restart=unless-stopped --env="constraint:node==frontend01" -d -p 5000:80 -e FRONTEND_NUM='01' --name web01 --net mynet --hostname votingapp.local web-vote-app

docker run --restart=unless-stopped --env="constraint:node==frontend02" -p 6379:6379 --name redis02 --net mynet -d redis  ## expose 6379 for debugging only!
docker run --restart=unless-stopped --env="constraint:node==frontend02" -d -p 5000:80 -e FRONTEND_NUM='02' --name web02 --net mynet --hostname votingapp.local web-vote-app
# [etc... for each web+redis node]

#
# Registrator:  after starting N redis's:
#
curl 192.168.33.11:8500/v1/catalog/service/redis | jq
[
  {
    "Node": "consul",
    "Address": "172.17.0.2",
    "ServiceID": "91b30bd7e369:redis01:6379",
    "ServiceName": "redis",
    "ServiceTags": null,
    "ServiceAddress": "10.0.0.2",
    "ServicePort": 6379
  },
  {
    "Node": "consul",
    "Address": "172.17.0.2",
    "ServiceID": "087acb552df3:redis02:6379",
    "ServiceName": "redis",
    "ServiceTags": null,
    "ServiceAddress": "10.0.0.3",
    "ServicePort": 6379
  }
]


#
# To vote, browse to:
#
http://votingapp.local

#
# To verify votes are going into the redis queue:
#
redis-cli -h 192.168.33.20
> llen votes
> lindex votes 0
> lindex votes 1
(etc)

#
# To view ha_proxy stats, browse to:
#
http://stats:interlock@votingapp.local/haproxy?stats

#
# To start postgres container:
#
docker run --restart=unless-stopped --env="constraint:node==pg" --name pg -e POSTGRES_PASSWORD=pg8675309 --net mynet -p 5432:5432 -d postgres ## expose 5432 for debugging only

And count votes like this:

export PSQL=/Applications/Postgres.app/Contents/Versions/9.5/bin/psql
PGPASSWORD=pg8675309 $PSQL -p5432 -h 192.168.33.251 -U postgres -d postgres 
#[and query like:  SELECT * FROM votes;]


#
# To start workers:
#
docker run --restart=unless-stopped --env="constraint:node==worker01" -d -e WORKER_NUMBER='01' -e FROM_REDIS_HOST=1 -e TO_REDIS_HOST=2 --name worker01 --net mynet vote-worker
#[etc... for each worker]

#
# To start results app
#
docker run --restart=unless-stopped --env="constraint:node==pg" -p 80:80 -d --name results-app --net mynet results-app

