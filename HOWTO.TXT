#!/bin/sh   # for syntax highlighting, not a script

###################################################
#
# Prereqs for Vagrant
#
###################################################

# You need the vagrant-reload provisioning plugin
$ vagrant plugin install vagrant-reload

$ vagrant up


###################################################
#
# Prereqs for AWS
#
###################################################

Go to AWS/HOWTO.txt.  Do steps listed there.
Then continue with the rest of the steps in this file.


###################################################
#
# Commands to create the Swarm cluster layer
#
###################################################

# Start the kv (for node discovery and libnetwork): 
docker -H=tcp://192.168.33.11:2375 run --restart=unless-stopped -d -p 8500:8500 -p 8400:8400 -p 8600:53/udp --name consul -h consul progrium/consul -server -bootstrap -ui-dir /ui
# web ui on:  http://192.168.33.11:8500/ui/

# Start the Swarm manager
docker -H=tcp://192.168.33.11:2375 run --restart=unless-stopped -d -p 3375:2375 swarm manage --discovery-opt="kv.path=docker/nodes" consul://192.168.33.11:8500/

# Create the overlay network
export DOCKER_HOST="tcp://192.168.33.11:3375"
docker network create --driver overlay mynet


###################################################
#
# Build instructions; same on AWS and Vagrant
#
###################################################

#
# To build web-vote-app:
#
cd ..
cd web-vote-app
docker rmi web-vote-app
docker -H tcp://192.168.33.20:2375 build -t web-vote-app .
docker -H tcp://192.168.33.21:2375 build -t web-vote-app .
# [etc... build for each web node because `docker build` on swarm master won't put image on every machine]
docker -H tcp://192.168.33.99:2375 build -t web-vote-app .

#
# To build worker:
#
cd ..
cd vote-worker
docker rmi vote-worker
docker -H tcp://192.168.33.200:2375 build -t vote-worker .
docker -H tcp://192.168.33.201:2375 build -t vote-worker .
# [etc... build for each worker node because `docker build` on swarm master won't put image on every machine]
docker -H tcp://192.168.33.249:2375 build -t vote-worker .

#
# To put redis image on each web node:
#
docker -H tcp://192.168.33.20:2375 pull redis
docker -H tcp://192.168.33.21:2375 pull redis
# [etc... for each web+redis node]
docker -H tcp://192.168.33.99:2375 pull redis

#
# To build results-app (which runs on pg machine)
#
cd ..
cd results-app
docker -H tcp://192.168.33.251:2375 build -t results-app .


###################################################
#
# DNS setup:  do on machine from which you browse
#
###################################################

#
# Vagrant:  point votingapp.local -> interlock on .12
#
vi /etc/hosts
Add this line:
	192.168.33.12   votingapp.local

#
# AWS:  point votingapp.local -> public IP of interlock (found in Outputs tab of CloudFormation)
#
vi /etc/hosts
Add this line:
	<public IP of interlock>   votingapp.local


###################################################
#
# Start application containers
#
###################################################

#
# AWS and Vagrant:  start Interlock on dedicated machine .12 (but listening to swarm master on .11):
#
docker -H tcp://192.168.33.12:2375 run --restart=unless-stopped -p 80:80 --name interlock -d ehazlett/interlock --swarm-url tcp://192.168.33.11:3375 --plugin haproxy start

#
# To run web+redis containers:
#
docker run --restart=unless-stopped --env="constraint:node==frontend01" -p 6379:6379 --name redis01 --net mynet -d redis  ## expose 6379 for debugging only!
docker run --restart=unless-stopped --env="constraint:node==frontend01" -d -p 5000:80 -e FRONTEND_NUM='01' --name web01 --net mynet --hostname votingapp.local web-vote-app

docker run --restart=unless-stopped --env="constraint:node==frontend02" -p 6379:6379 --name redis02 --net mynet -d redis  ## expose 6379 for debugging only!
docker run --restart=unless-stopped --env="constraint:node==frontend02" -d -p 5000:80 -e FRONTEND_NUM='02' --name web02 --net mynet --hostname votingapp.local web-vote-app

# [etc... for each web+redis node]

#
# To start workers:
#
docker run --restart=unless-stopped --env="constraint:node==worker01" -d -e WORKER_NUMBER='01' -e REDIS_PREFIX="redis" -e REDIS_MAX=10 --name worker01 --net mynet vote-worker
#[etc... for each worker]

#
# To start results app
#
docker run --restart=unless-stopped --env="constraint:node==pg" -p 80:80 -d --name results-app --net mynet results-app

#
# To start postgres container:
#
docker run --restart=unless-stopped --env="constraint:node==pg" --name pg -e POSTGRES_PASSWORD=pg8675309 --net mynet -p 5432:5432 -d postgres ## expose 5432 for debugging only


###################################################
#
# Usage and debugging
#
###################################################

#
# To vote, browse to:
#
http://votingapp.local

#
# To verify votes are going into the redis queue:
#
redis-cli -h 192.168.33.20
> llen votes
> lindex votes 0
> lindex votes 1
(etc)

#
# To view ha_proxy stats, browse to:
#
http://stats:interlock@votingapp.local/haproxy?stats

#
# And count votes like this:
#
export PSQL=/Applications/Postgres.app/Contents/Versions/9.5/bin/psql
PGPASSWORD=pg8675309 $PSQL -p5432 -h 192.168.33.251 -U postgres -d postgres 
#[and query like:  SELECT * FROM votes;]

#
# Or view them in results-app:
#
http://192.168.33.251:80


